<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Jianchao Li">

  
  
  
    
  
  <meta name="description" content="I used Captum to interpre the output of a MobileNetV2, which visualized the main regions in the input image that drove the model to generate its output.">

  
  <link rel="alternate" hreflang="en-us" href="https://jianchao-li.github.io/post/interpret-pytorch-models-with-captum/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.aebf6ad474c2ff50ae0b78ca18c392cf.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-149941944-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://jianchao-li.github.io/post/interpret-pytorch-models-with-captum/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Jianchao Li">
  <meta property="og:url" content="https://jianchao-li.github.io/post/interpret-pytorch-models-with-captum/">
  <meta property="og:title" content="Interpret PyTorch Models with Captum | Jianchao Li">
  <meta property="og:description" content="I used Captum to interpre the output of a MobileNetV2, which visualized the main regions in the input image that drove the model to generate its output."><meta property="og:image" content="https://jianchao-li.github.io/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2020-02-29T10:49:18&#43;08:00">
  <meta property="article:modified_time" content="2020-02-29T10:49:18&#43;08:00">
  

  


  





  <title>Interpret PyTorch Models with Captum | Jianchao Li</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Jianchao Li</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#experience">
            
            <span>Experience</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#accomplishments">
            
            <span>Accomplishments</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Interpret PyTorch Models with Captum</h1>

  

  
    



<meta content="2020-0229T10:49:18&#43;08:00" itemprop="datePublished">
<meta content="2020-02-29 10:49:18 &#43;0800 &#43;0800" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    <time>Feb 29, 2020</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  

  
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Interpret%20PyTorch%20Models%20with%20Captum&amp;url=https%3a%2f%2fjianchao-li.github.io%2fpost%2finterpret-pytorch-models-with-captum%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fjianchao-li.github.io%2fpost%2finterpret-pytorch-models-with-captum%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjianchao-li.github.io%2fpost%2finterpret-pytorch-models-with-captum%2f&amp;title=Interpret%20PyTorch%20Models%20with%20Captum"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fjianchao-li.github.io%2fpost%2finterpret-pytorch-models-with-captum%2f&amp;title=Interpret%20PyTorch%20Models%20with%20Captum"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Interpret%20PyTorch%20Models%20with%20Captum&amp;body=https%3a%2f%2fjianchao-li.github.io%2fpost%2finterpret-pytorch-models-with-captum%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <p>While deep neural networks have achieved state-of-the-art performance in many problems(e.g., image classification, object detection, scene parsing etc.), it is always not trivial to intepret their outputs. Till now, the most common and useful way to interpret the output of a deep neural network is still by visualization. You may refer to this <a href="http://cs231n.github.io/understanding-cnn/">CS231n course note</a> for some introduction.</p>
<p>In this post, I will describe how to interpret an image classification model using <a href="https://captum.ai/">Captum</a>. Captum, which means &ldquo;comprehension&rdquo; in Latin, is a open-source project with many model interpretabiliy algorithms implemented in PyTorch. Specifically, I adopted <a href="https://github.com/pytorch/captum/blob/master/captum/attr/_core/layer/grad_cam.py#L21"><code>LayerGradCam</code></a> for this post.</p>
<h2 id="install-captum">Install Captum</h2>
<p>As <code>LayerGradCam</code> is still not released at the time of writing this post, to use it, clone the Captum repository locally and install it from there.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>git clone git@github.com:pytorch/captum.git
</span></span><span style="display:flex;"><span>cd captum
</span></span><span style="display:flex;"><span>pip install -e .
</span></span></code></pre></div><p>Then import all the required packages.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> json
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> requests
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> io <span style="color:#f92672">import</span> BytesIO
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> cv2
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torchvision <span style="color:#f92672">import</span> models, transforms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> PIL <span style="color:#f92672">import</span> Image
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>matplotlib inline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> captum.attr <span style="color:#f92672">import</span> LayerAttribution, LayerGradCam
</span></span></code></pre></div><h2 id="prepare-a-model-and-an-image">Prepare a Model and an Image</h2>
<p>I use the <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenet.py#L72"><code>MobileNetV2</code></a> pretrained on ImageNet from <code>torchvision</code> and an image of a Hornbill from <a href="https://upload.wikimedia.org/wikipedia/commons/8/8f/Buceros_bicornis_%28female%29_-feeding_in_tree-8.jpg">Wikipedia</a>. Later I will use <code>LayerGradCam</code> to intepret and visualize why the model gives the specific output for this image.</p>
<p>Note that the model needs to be set to the test mode.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># use MobileNetV2</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> models<span style="color:#f92672">.</span>mobilenet_v2(pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>eval()
</span></span></code></pre></div><p>For the image, I first read its encoded string from its URL and then use the <code>PIL.Image</code> format to decode it. In this way, the channels of the image are in the RGB order.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>img_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;https://upload.wikimedia.org/wikipedia/commons/8/8f/Buceros_bicornis_</span><span style="color:#e6db74">%28f</span><span style="color:#e6db74">emale%29_-feeding_in_tree-8.jpg&#39;</span>
</span></span><span style="display:flex;"><span>resp <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>get(img_url)
</span></span><span style="display:flex;"><span>img <span style="color:#f92672">=</span> Image<span style="color:#f92672">.</span>open(BytesIO(resp<span style="color:#f92672">.</span>content))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">10</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>imshow(img)
</span></span></code></pre></div><p><img src="/post/interpret-pytorch-models-with-captum/image.png"></p>
<p>I also prepare the class names for the 1000 classes in ImageNet. This will let me know the specific class names instead of only the index of the predicted class. The class names are loaded from the following URL.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&#39;</span>
</span></span><span style="display:flex;"><span>resp <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>get(url)
</span></span><span style="display:flex;"><span>class_names_map <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>loads(resp<span style="color:#f92672">.</span>text)
</span></span></code></pre></div><h2 id="preprocessing">Preprocessing</h2>
<p>For <code>torchvision</code> models, before passing an image to it, the image needs to be applied the following preprocessing (<a href="https://github.com/pytorch/vision/issues/39#issuecomment-403701432">reference</a>). This is a key step to make the model run on images from the same distribution as of those that it was trained on.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>preprocessing <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose([
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>Resize(<span style="color:#ae81ff">256</span>),
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>CenterCrop(<span style="color:#ae81ff">224</span>),
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>ToTensor(),
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>Normalize(
</span></span><span style="display:flex;"><span>        mean<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.485</span>, <span style="color:#ae81ff">0.456</span>, <span style="color:#ae81ff">0.406</span>],
</span></span><span style="display:flex;"><span>        std<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.229</span>, <span style="color:#ae81ff">0.224</span>, <span style="color:#ae81ff">0.225</span>]
</span></span><span style="display:flex;"><span>    ),
</span></span><span style="display:flex;"><span>])
</span></span></code></pre></div><h2 id="layergradcam">LayerGradCam</h2>
<p>Now we can apply <code>LayerGradCam</code> to &ldquo;attribute&rdquo; the output of the model to a specific layer of the model. What <code>LayerGradCam</code> does is basically computing the gradients of the output with respect to that specific layer. The following function is used to get a layer from the model by its name.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_layer</span>(model, layer_name):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> layer_name<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;.&#34;</span>):
</span></span><span style="display:flex;"><span>        model <span style="color:#f92672">=</span> getattr(model, name)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> model
</span></span></code></pre></div><p>The <code>features.18</code> layer of MobileNetV2 will be used in this notebook.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>layer <span style="color:#f92672">=</span> get_layer(model, <span style="color:#e6db74">&#39;features.18&#39;</span>)
</span></span></code></pre></div><p>We will use <code>LayerGradCam</code> to compute the attribution map (gradients) of the model&rsquo;s top-1 output with respect to <code>layer</code>. This map can be interpreted as to what extent is the output influenced by a unit in <code>layer</code>. This makes sense as the larger the gradient, the larger the influence.</p>
<p>This attribution map (with the same size as the output of <code>layer</code>, in this case, 7*7) is further upsampled to the size of the image and overlaid on the image as a heatmap. So this heatmap reflects how much influence each pixel has on the output of the model. The pixels with larger influence (the red regions in the heatmap) can thus be interpreted as the main regions in the image that drive the model to generate its output.</p>
<p>To enable all above processing of the attribution map, two functions are implemented as follows. The first function <code>to_gray_image</code> converts an <code>np.array</code> to a gray-scale image by normalizing its values to <code>[0, 1]</code>, multiplying it by 255, and converting its data type to <code>uint8</code>. The second one <code>compute_heatmap</code> utilizes <code>cv2</code> to overlay a <code>torch.Tensor</code> as a heatmap on an image.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">to_gray_image</span>(x):
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">-=</span> x<span style="color:#f92672">.</span>min()
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">/=</span> x<span style="color:#f92672">.</span>max() <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>spacing(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">*=</span> <span style="color:#ae81ff">255</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>array(x, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>uint8)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">overlay_heatmap</span>(img, grad):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># convert PIL Image to numpy array</span>
</span></span><span style="display:flex;"><span>    img_np <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(img)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># convert gradients to heatmap</span>
</span></span><span style="display:flex;"><span>    grad <span style="color:#f92672">=</span> grad<span style="color:#f92672">.</span>squeeze()<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>    grad_img <span style="color:#f92672">=</span> to_gray_image(grad)
</span></span><span style="display:flex;"><span>    heatmap <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>applyColorMap(grad_img, cv2<span style="color:#f92672">.</span>COLORMAP_JET)
</span></span><span style="display:flex;"><span>    heatmap <span style="color:#f92672">=</span> heatmap[:, :, ::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#75715e"># convert to rgb</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># overlay heatmap on image</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> cv2<span style="color:#f92672">.</span>addWeighted(img_np, <span style="color:#ae81ff">0.5</span>, heatmap, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>In <code>overlay_heatmap</code>, note that <code>img</code> is in RGB order while the <code>heatmap</code> returned by <code>cv2.applyColorMap</code> is in BGR order. So we convert <code>heatmap</code> to RGB order first before the overlay.</p>
<p>Using all above functions, the following function <code>attribute</code> computes and overlays the <code>LayerGradCam</code> heatmap on an image.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">attribute</span>(img):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># preprocess the image</span>
</span></span><span style="display:flex;"><span>    preproc_img <span style="color:#f92672">=</span> preprocessing(img)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># forward propagation to get the model outputs</span>
</span></span><span style="display:flex;"><span>    inp <span style="color:#f92672">=</span> preproc_img<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    out <span style="color:#f92672">=</span> model(inp)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># construct LayerGradCam</span>
</span></span><span style="display:flex;"><span>    layer_grad_cam <span style="color:#f92672">=</span> LayerGradCam(model, layer)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># generate attribution map</span>
</span></span><span style="display:flex;"><span>    _, out_index <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(out, k<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    out_index <span style="color:#f92672">=</span> out_index<span style="color:#f92672">.</span>squeeze(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    attr <span style="color:#f92672">=</span> layer_grad_cam<span style="color:#f92672">.</span>attribute(inp, out_index)
</span></span><span style="display:flex;"><span>    upsampled_attr <span style="color:#f92672">=</span> LayerAttribution<span style="color:#f92672">.</span>interpolate(attr, (img<span style="color:#f92672">.</span>height, img<span style="color:#f92672">.</span>width), <span style="color:#e6db74">&#39;bicubic&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># generate heatmap</span>
</span></span><span style="display:flex;"><span>    heatmap <span style="color:#f92672">=</span> overlay_heatmap(img, upsampled_attr)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> heatmap, out_index<span style="color:#f92672">.</span>item()
</span></span></code></pre></div><p>Specifically, what <code>attribute</code> does is as follows.</p>
<ol>
<li>Preprocess the image;</li>
<li>Run a forward propagation on the image to get the model&rsquo;s output;</li>
<li>Construct a <code>LayerGradCam</code> object using <code>model</code> and <code>layer</code>;</li>
<li>Generate the attribution map of the model&rsquo;s top-1 output to <code>layer</code>;</li>
<li>Upsample the attribution map to the same size as the image;</li>
<li>Overlay the attribution map as a heatmap on the image.</li>
</ol>
<p>Now it is time to run an example! Let&rsquo;s see what class the <code>model</code> predicts on the Hornbill image, and more importantly, why.</p>
<pre tabindex="0"><code>vis, out_index = attribute(img)
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
ax.set_title(class_names_map[str(out_index)], fontsize=30)
plt.imshow(vis)
</code></pre><p><img src="/post/interpret-pytorch-models-with-captum/visualization.png"></p>
<p>We can see that the model makes a correct prediction. From the above visualization, we can also see that the red regions are mostly around the head and beak of the Hornbill, especiall its heavy bill. The red regions are the main regions that drive the model to generate its output. This makes great sense as those regions are just the distinctive features of a Hornbill.</p>
<p>Now you can also apply the above technique (and more from Captum) to interpret the output of your PyTorch model. Have fun!</p>
<p><strong>Notes:</strong> This post is alao avaialble as a <a href="https://gist.github.com/jianchao-li/f7b507bc66b2215e15cc0135f03c3ff9">Jupyter notebook</a>.</p>

    </div>

    



    
      








  







      
      
    

    

    


  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.7720ca23a3e05277ade871a05f8331cb.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    Â© 2025 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
